%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
\section{The Model Building Process}
Machine learning models have been used in to solve problems in a large number of
domains. They can be used to recommend movies~\cite{netflixprize}, 
classify tumors as benign or malignant~\cite{tumorprediction}, recognize faces~\cite{deepface}, and more. 
Building these highly predictive machine learning models, however, is not easy. 

A data scientist engaged in the model building process goes through many cycles of
experimentation. For example, in a single cycle, a data scientist may perform the
following steps:

\begin{enumerate}
  \item Apply preprocessing operations to the data.
  \item Split the data into training and test sets.
  \item Select a model type and optimization criterion for training.
  \item Define a hyperparameter search grid for the model.
  \item Use cross validation to evaluate hyperparameter configurations and select a 
    promising model.
  \item Evaluate the model on the test set
\end{enumerate}

Over many cycles, the data scientist will have created and evaluated many models,
applied various preprocessing pipelines, and tried numerous feature sets and hyperparameter
configurations. Currently, recording the operations performed in these cycles requires
a lot of manual effort. The data scientist must write their intermediate data files, models, evaluation metrics,
and preprocessing code to different files. They may invent some ad-hoc directory layout 
and a set of naming conventions to organize this information~\cite{kaggledirectory}. Even if the data scientist
diligently records all their cycles, it is not easy for them to glean information from their
recorded data without writing some custom scripts.

Recording information about the model building process can help the data scientist 
make better decisions on improving model performance. Comparing hyperparameters
and feature sets may provide insight into why two models differ in performance. 
Examining the most important features may highlight what parts of the dataset are
most useful for prediction. Tracing a model's lineage can show what preprocessing
steps were most effective. Storing evaluation metrics for models can help
the data scientist focus on only the highest quality models. 

Recording information about the model building process can answer some useful 
questions, but currently it is difficult and time-consuming to do.

\section{ModelDB Server and ModelDB Spark Client}

ModelDB Server and ModelDB Spark Client aim to simplify the challenge of recording, storing,
and learning from the model building process. 

ModelDB Spark Client is a lighweight library for Apache Spark's machine learning framework (Spark.ML) 
that allows the user to collect information about their machine learning operations with minimal changes to their code. 

The Spark Client sends the information it collects to ModelDB Server, which 
stores the operations and models in a centralized database and the serialized 
models in its filesystem. 

The abstractions used in ModelDB Server are library agnostic. This means ModelDB Server 
and its database schema are not tied to any particular machine learning library, making
it possible to develop clients for other popular machine learning libraries like Python Scikit-learn.

ModelDB Server also exposes an API that allows the user to glean information about their
operations and models. This allows users to compare models, find
similar models, rank models, fetch the steps that produced a model, and more. 

While ModelDB Server has functionality for storing and extracting information from
all kinds of models, it stores extra information and provides richer APIs for 
Logistic Regression, Linear Regression, Decision Tree, Random Forest, and 
Gradient Boosted Tree models. 

Concretly, this thesis makes the following contributions:

\begin{enumerate}
  \item A set of library-agnostic abstractions that can be used to
    represent a wide range of machine learning operations and models.
  \item A number of algorithms that can be applied on the above abstractions
    to gain insight into the model building process.
  \item A working system that can both store data about 
    machine learning operations and models (especially linear and tree models) 
    as well as extract useful information from this data.
  \item A working client library for Spark ML that can collect information 
    about various  machine learning operations and models with 
    minimal required changes to a data scientist's code.
\end{enumerate}


\section{Outline of this Paper}

Chapter 2 covers related work. It first discusses machine learning in general,
exposing the key concepts that are relevant in the model building process. The
discussion then shifts to linear and tree models in particular, because ModelDB
Server and Spark Client have special support for these kinds of models. Next, Spark
and Spark ML are briefly described. The chapter then proceeds to discuss popular
machine libraries and their abstractions and concludes by discussing other machine
learning support systems.

Chapter 3 covers the abstractions used in ModelDB Server and ModelDB Spark Client.
It begins with a discussion of Transformer, TransformerSpec, and DataFrame, which 
the primitives that are used to create the other abstractions. Next, the chapter
introduces the Syncable Event abstraction, which represents a machine learning
operation that can be logged. Then, some of the simple Syncable Events, like FitEvent,
which represents the fitting of a model, are described. Next, more complicated Syncable
Events, like PipelineEvent are discussed. The chapter then moves on to discuss the
abstractions for representing linear and tree models. Finally, the ModelDBSyncer,
a client side abstraction for sending events to ModelDB Server, is discussed.

Chapter 4 covers algorithms used in ModelDB Server and ModelDB Spark Client. It 
begins with storage algorithms which capture the complexities involved in storing
and connecting the Syncable Events. The next topic is ancestry algorithms, which
operate on the ancestry chain of DataFrames. The chapter then talks about algorithms
that operate on the feature sets of models and and algorithms that compare, rank,
and group models. Finally, the chapter discusses algorithms for linear models
and tree models.

Chapter 5 covers implementation. It starts with an overall view of the system,
including ModelDB Server, the database, the model filesystem, Spark, and more. 
Then, attention shifts to an outline of the ModelDB Server's implementation. Next,
the chapter discusses the Spark Client implementation. The chapter concludes with
a discussion of how applications can be built on top of ModelDB Server.

Chapter 6 is the evaluation. It first describes an experiment run to measure the
time and space overhead of collecting operations and models from Spark and recording
them in the ModelDB Server. The chapter then describes an experiment to measure
the time taken to run some of the more complicated API methods exposed by ModelDB Server.
Finally, the chapter presents the results of the experiments and discusses potential performance
improvements.

Chapter 7 describes the role of ModelDB Server and ModelDB Spark Client in ModelDB,
a larger system that includes other components like a client library for Scikit-learn,
a command line toolkit, a prediction store, and a visualization application.

Chapter 8 covers potential future work that could be done on ModelDB Server and Spark Client.

Chapter 9 concludes the paper.
