%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
\section{The Model Building Process}
Machine learning models have been used to solve problems in a large number of
domains. They can be used to recommend movies~\cite{netflixprize}, 
classify tumors as benign or malignant~\cite{tumorprediction}, recognize faces~\cite{deepface}, and more. 
Building these highly predictive machine learning models, however, is not easy. 

A data scientist engaged in the model building process goes through many cycles of
experimentation. For example, in a single cycle, a data scientist may perform the
following steps:

\begin{enumerate}
  \item Apply preprocessing operations to the data.
  \item Split the data into train and test sets.
  \item Select a model type and optimization criterion for training.
  \item Define a hyperparameter search grid for the model.
  \item Use cross validation to evaluate hyperparameter configurations and select a 
    promising model.
  \item Evaluate the model on the test set.
\end{enumerate}

Over many cycles, the data scientist will create and evaluate many models,
apply various preprocessing pipelines, and try numerous feature sets and hyperparameter
configurations. Currently, recording the operations performed in these cycles requires
a lot of manual effort. The data scientist must write their intermediate data files, models, evaluation metrics,
and preprocessing code to different files. They may invent some ad-hoc directory layout 
and naming conventions to organize this information~\cite{kaggledirectory}. Even if the data scientist
diligently records all their cycles, it is not easy for them to glean information from their
recorded data without writing some custom scripts.

Recording information about the model building process can help the data scientist 
make better decisions on improving model performance. Comparing hyperparameters
and feature sets may provide insight into why two models differ in performance. 
Examining the most important features may highlight what parts of the dataset are
most useful for prediction. Tracing a model's lineage can show what preprocessing
steps were most effective. Storing evaluation metrics for models can help
the data scientist focus on only the highest quality models. 

Recording information about the model building process can answer some useful 
questions, but currently it is difficult and time-consuming to do.

\section{ModelDB Server and ModelDB Spark Client}

This challenge of recording, storing, and learning from the model building process is precisely
the problem that ModelDB Server and ModelDB Spark Client aim to solve.

ModelDB Spark Client is a lightweight library for Apache Spark's machine learning library (Spark.ML) 
that allows the user to collect information about their machine learning operations with minimal changes to their code. 

The Spark Client sends the information it collects to ModelDB Server, which 
stores the operations and models in a centralized database and the serialized 
models in its filesystem. 

The abstractions used in ModelDB Server are library agnostic. This means ModelDB Server 
and its database schema are not tied to any particular machine learning library, making
it possible to develop clients for other popular machine learning libraries like Scikit-learn for Python.

ModelDB Server also exposes an API that allows the user to glean information about their
operations and models. This allows users to compare models, find
similar models, rank models, fetch the steps that produced a model, and more. 

While ModelDB Server has functionality for storing and extracting information from
all kinds of models, it stores extra information and provides richer APIs for 
logistic regression, linear regression, decision tree, random forest, and 
gradient boosted tree models. 

Concretely, this thesis makes the following contributions:

\begin{enumerate}
  \item A set of library-agnostic abstractions that can be used to
    represent a wide range of machine learning operations and models.
  \item A number of algorithms that can be applied on the above abstractions
    to gain insight into the model building process.
  \item A working system that can both store data about 
    machine learning operations and models (especially linear and tree models) 
    as well as extract useful information from this data.
  \item A working client library for Spark ML that can collect information 
    about various  machine learning operations and models with 
    minimal required changes to a data scientist's code.
\end{enumerate}

For the rest of this paper, ModelDB Server and the Spark Client will be referred
to collectively as ModelDB S+C.

\section{Usage Scenario}
To motivate why ModelDB S+C would be useful, consider the usage scenario
described below.

Data scientist Alice is trying to build a model that can predict the click through rate
of an advertisement based on various features of the advertisement (e.g. target demographic,
color scheme). She uses Spark and ModelDB S+C. Alice first applies various preprocessing steps to
her data, which are recorded and stored in ModelDB Server. At one point, she realizes that there
is a bug in one of her dataset's columns, so she uses ModelDB Server's API to determine
all the operations she has applied to her dataset. With this information, she realizes she forgot a preprocessing step, so
she makes sure to apply it.
Next, Alice trains a few models on the dataset and computes various evaluation metrics on them. Using
ModelDB Server's API, she can rank these models along various metrics and identify which one performed
best overall. Alice selects the best model and uses ModelDB Server's API to 
rank its features by importance. She sees that it is one of the few 
models she created that uses the "color scheme" feature, so she suspects 
that this feature may be important. So, Alice uses ModelDB S+C's annotation API and 
annotates the model with the text "Model works really well, I think the color scheme feature is
important for accurate prediction" so that her colleagues will be informed if they later train
similar models. Alice trains a new model and finds that its performance
is worse. To debug the issue, she uses ModelDB Server's API to compare the model
to the best one she has trained so far. She sees that there's a difference in the number of iterations
the models were trained for. Alice trains the new model for a greater number of iterations and finds
that the performance of the model has greatly improved. Happy with the results, she uses ModelDB's visualization
tool (part of the overall ModelDB system, it is built on ModelDB Server) to graphically view all the operations 
she performed so that she can refer to it as she writes about her process and shares her results with her colleagues.

Later, data scientist Bob comes along and is also looking for a system to predict 
ad click through rate. He uses ModelDB Server's API to find all the models that 
descended from the ads dataset. He uses ModelDB Server's API to group these models
based on their problem type, and he focuses on the ones that were used for predicting click
through rate. Then, he uses ModelDB Server's API to rank these models and find the best performing
model. The best performing model turns out to be one that would be difficult to scale for Bob's
use case, so he uses ModelDB Server's API to find a similar model to the best one. This yields
Alice's model, which is similar in performance, but much easier to scale. ModelDB S+C stores serialized models, so
Bob loads Alice's model and uses it to make a few predictions. Happy with the results, Bob
decides to evaluate the model on the latest installment of the ads dataset. However, he doesn't
know how to preprocess the data. Fortunately, Bob is able to use ModelDB Server's API to extract the 
preprocessing pipeline that was applied to the model's original dataset, and he loads each of
the data preprocessors and preprocesses his data. Then, he feeds in his dataset and evaluates
the model. Happy with its performance, he decides to use it in his system.

\section{Outline of this Thesis}

Chapter 2 covers related work. It first discusses machine learning in general,
exposing the key concepts that are relevant in the model building process. The
discussion then shifts to linear and tree models in particular, because ModelDB S+C has 
special support for these kinds of models. 
Next, Spark and Spark ML are briefly described. The chapter then proceeds to discuss popular
machine learning libraries and their abstractions and concludes by discussing other machine
learning support systems.

Chapter 3 covers the abstractions used in ModelDB S+C.
It begins with a discussion of Transformer, TransformerSpec, and DataFrame, which are
the primitives that are used to create the other abstractions. Next, the chapter
introduces the Syncable Event abstraction, which represents a machine learning
operation that can be logged. Then, some of the simple Syncable Events, like FitEvent,
which represents the fitting of a model, are described. Next, more complicated Syncable
Events, like PipelineEvent are discussed. The chapter then moves on to discuss the
abstractions for representing linear and tree models. Finally, the ModelDB Syncer,
a client side abstraction for sending events to ModelDB Server, is discussed.

Chapter 4 covers algorithms used in ModelDB S+C. It 
begins with storage algorithms that capture the complexities involved in storing
and connecting the Syncable Events. The next topic is ancestry algorithms, which
operate on the ancestry forest of DataFrames. The chapter then talks about algorithms
that operate on the feature sets of models and algorithms that compare, rank,
and group models. Finally, the chapter discusses algorithms for linear models
and tree models.

Chapter 5 covers implementation. It starts with an overall view of the system,
including ModelDB Server, the database, the model filesystem, Spark, and more. 
Then, attention shifts to an outline of ModelDB Server's implementation. Next,
the chapter discusses the Spark Client's implementation. The chapter concludes with
a discussion of how applications can be built on top of ModelDB Server.

Chapter 6 is the evaluation. It first describes an experiment run to measure the
time and space overhead of collecting operations and models from Spark and recording
them in ModelDB Server. The chapter then describes an experiment to measure
the time taken to run some of the more complicated API methods exposed by ModelDB Server.
The chapter then considers real machine learning workflows, and examines how well ModelDB S+C is
able to capture the operations performed.
Finally, the chapter discusses potential performance
improvements.

Chapter 7 describes the role of ModelDB S+C in ModelDB,
a larger system that includes other components like a client library for Scikit-learn,
a command line toolkit, a prediction store, and a visualization application.

Chapter 8 covers potential future work that could be done on ModelDB S+C.

Chapter 9 concludes the thesis.
