\chapter{Evaluation}
By importing ModelDBSyncer and appending "Sync" to the method names, the user
can use ModelDB Spark Client to record and send their operations and models
on ModelDB Server, which stores the data in its database and model filesystem.
Naturally, this takes extra running time and storage space than if the user did
not use ModelDB S+C at all. Additionally, the API methods exposed by ModelDB Server
take some time to run too. The goal of this chapter is to measure whether the time
and space requirements of ModelDB S+C are reasonable.


\section{Overview}
Experiments were conducted to answer the following questions:

\begin{enumerate}
\item Is the time overhead of storing operations and models on ModelDB Server reasonably small (compared to
the overall running time of the Spark.ML program)?
\item Is the space taken up by ModelDB S+C's database and model files reasonably small (compared
to the size of the dataset)?
\item Are ModelDB S+C's tree model and linear model representations reasonably small (compared 
to the size of corresponding PMML files)?
\item Is the time take to execute the ModelDB Server API methods reasonably small (compared
to the training time of the model)?
\item Can ModelDB S+C record most of the machine learning operations in real Spark.ML programs?
\end{enumerate}

The chapter also discusses performance improvements that can further improve ModelDB S+C with
regard to the above questions.

\section{Datasets}
 \begin{table}
   \centering
    \begin{tabular}{ | l | l | l | l |}
      \hline
      Dataset Name & Problem Type & Number of Rows & Number of Features \\ \hline
      IMDB & Regression & 5,043 & 28 \\ \hline
      Animal Shelter & Multiclass Classification & 26,729 & 10 \\ \hline
      Housing Prices & Regression & 1,460 & 81 \\ \hline
      Iris & Multiclass Classification & 150 & 5 \\  \hline
      Titanic & Binary Classification & 1,309 & 14  \\  \hline
      SMS Spam & Binary Classification & 5,574 & N/A (text) \\  \hline
      Flight Delays & Binary Classification & 7,453,215 & 29 \\ 
      \hline
   \end{tabular}
   \caption{Datasets used in evaluation}
   \label{tab:datasets}
 \end{table}

ModelDB S+C was evaluated on real datasets, which are described below and outlined
in Table \ref{tab:datasets}.

The \textbf{IMDB} dataset \cite{imdb} includes features like genre, number of
reviews, language, and more for over 5000 movies in the IMDB Movie Database. The dataset
also includes the IMDB score (i.e. a 1 to 10 rating of how good the movie is considered)
for each movie. A machine learning model could solve a regression problem on this
dataset in which it predicts the IMDB score for a given movie. Such a model could
be used to identify the best movies and recommend them to users. This dataset
was used when measuring the time/space overhead of ModelDB S+C and was also used
when measuring the execution time for ModelDB Server API methods.

The \textbf{Animal Shelter} dataset \cite{animal} includes features like
animal type (cat or dog), breed, color, age, and more for over 25,000 animals. The
dataset also includes the animal's outcome (e.g. adopted, returned to owner, transferred)
for each animal. A machine learning model could solve a multi-class classification problem on this dataset
in which it predicts the most likely outcome for a given animal. Such a model could 
be used to identify the most likely outcome for an animal newly admitted into the
an animal shelter. This dataset was used when measuring the time/space overhead
of ModelDB S+C.

The \textbf{Housing Prices} dataset \cite{housing} includes features like square
footage, number of bathrooms, neighborhood type, and more for about 1,500 houses.
The dataset also includes the sale price of each house. A machine learning model
could solve a regression problem on this dataset in which it predicts the sale
price of a given home. Such a model could be used by realtors who
are trying to find the best price for a home when putting it up on the market. This
dataset was used when measuring the time/space overhead of ModelDB S+C.

The \textbf{Iris} dataset \cite{iris} is a famous dataset that includes features
like petal length, sepal width, and more for 150 Iris plans. The dataset also
includes the species of the Iris plant. A machine learning model could solve a 
multi-class classification problem on this dataset in which it predicts the species
of a given Iris plant. Such a model may be useful for botanists looking to classify
their plants. This dataset was used when measuring the size of ModelDB S+C models
to PMML models (the PMML website includes model files for the Iris dataset).

The \textbf{Titanic} dataset \cite{titanic} is a famous dataset includes features like 
passenger sex, passenger class, and more for over 1300 passengers of the Titanic. 
The dataset also includes whether each passenger survived or died. A machine learning
model could solve a binary classification problem on this dataset in which it predicts whether
a given passenger survived. Such a model may be useful for historians. This dataset
was used when evaluating ModelDB S+C on an existing machine learning workflow.

The \textbf{SMS Spam} dataset \cite{spam} includes over 5500 text messages and
a label indicating whether the message is spam or not. A machine learning model
could solve a binary classification problem on this dataset in which it predicts
whether a given text message is spam. Such a model could be used as a text messaging
app's spam filter. This dataset was used when evaluating ModelDB S+C on an existing
machine learning workflow.

The \textbf{Flight Delays} dataset \cite{airline} includes features like carrier, departure
time, and more for over 14 million airplane departures. It also includes a field indicating whether
the plane was delayed. A machine learning model could solve a binary classification problem on this dataset in which
it predicts whether a given airplane is delayed. Such a model could be used by airports and airlines
to anticipate delays for particular airplanes. This dataset was used when evaluating ModelDB S+C
on an existing machine learning workflow.

\section{Methodology}
\subsection{Machine}
These experiments were run on a DigitalOcean machine with a 160 GB SSD disk, an 8 core processor,
and 16GB of memory.

\subsection{Time and Space Overhead}
The first experiment focused on evaluating the time and space overhead of ModelDB S+C.
The IMDB, Housing Prices, and Animal Shelter datasets were used for this experiment. 

First, for each of the datasets listed above, three programs (called workflows) were
created. The \textbf{simple} workflow simply trains and evaluates one machine learning model. 
The \textbf{full} workflow creates a preprocessing pipeline for the data, trains 
some models with grid search cross validation (thus trying many hyperparameter configurations), 
and evaluates the best model. The \textbf{exploratory} worklow executes many full workflows, 
trying different model types (e.g. random forest, linear regression) and different feature sets.

Second, a program was written to artificially increase the size of the dataset by duplicating rows.
Dataset sizes were varied from the dataset's original number of rows to one million rows.

Third, ModelDB Spark Client was instrumented so that it recorded the time spent running code related
to ModelDB S+C.

Fourth, each (dataset, dataset size, workflow) triple's program was executed with ModelDB S+C enabled. The time that ModelDB S+C spent
recording and storing each operation and model was recorded. The database size, number of rows in each table, and size of the model
files was also recorded. Finally, the overall running time of the program and the size of the (artificially enlarged) dataset
were also noted. Due to Spark's large memory requirements, it was not possible to run the exploratory workflow for the 
housing dataset when the number of rows was large because Spark.ML consumed too much memory. This is because Spark is designed
to be run on machines with large amounts of RAM, and the machine used for these experiments had only 16 GB of RAM.

\subsection{Computation Time of API Methods}
The second experiment focused on evaluating the running time of ModelDB Server's API methods. The
IMDB dataset was used here.

First, the IMDB exploratory workflow was run to populate ModelDB Server's database.

Second, the database was duplicated $N$ times (i.e. $N$ additional rows were created for
every row in every table of the database), where $N$ was varied from 0 to 400. This was
done to simulate many exploratory workflows being run on ModelDB S+C.

Third, various API methods were executed on the (artificially enlarged) database and the
running time for each method was recorded.

\subsection{Compare ModelDB S+C Models to PMML}
The PMML website \cite{pmmlwebsite} includes sample files for a 200 tree random forest model
and logistic regression model run that were trained on the Iris dataset. So, this
experiment trained a random forest model and a one vs. rest logistic regression model
in Spark.ML (with ModelDB S+C enabled). The sizes of the model files and database
were recorded.

\subsection{Evaluating Existing Workflows}
Three machine learning workflows were collected from the Internet. The first \cite{flightworkflow}, from
a Hortonworks article, uses the Flight Delays dataset to build a regression model to predict
delays. The second \cite{titanicworkflow} and third \cite{spamworkflow} are from ZeppelinHub, a website that allows users to share their
machine learning workflows as Zeppelin notebooks. The second workflow trains two models
to predict passenger survival status for the Titanic dataset and the third builds a spam classifier
using the SMS Spam dataset. These workflows were cleaned up, ported to Spark v2.0.0, and 
augmented with ModelDB Spark Client (i.e. import ModelDbSyncer and add *Sync to the methods).

\section{Time Overhead Results}
To measure the time overhead, the time spent running ModelDB S+C code is divided 
by the total running time of the program and the result is taken as a percentage.
Plotting this time overhead percentage, for each (dataset, workflow) pair,
as a function of the number of rows in the dataset yields Figure \ref{fig:time_overhead}.
For this experiment, ModelDB S+C did NOT count the number of rows in each DataFrame.

\begin{figure}
  \centering
  \includegraphics[width=6.0in]{time_overhead}
  \caption{
    Time overhead of ModelDB S+C.
  }
  \label{fig:time_overhead}
\end{figure}

Figure \ref{fig:time_overhead} shows that, as the dataset size grows, the time overhead
percentage for ModelDB S+C goes down. For a one million row dataset,
most of the workflows spend less than 5\% of their time running ModelDB S+C code.

For the results here, ModelDB S+C is configured not to count the number of rows in the DataFrames.
Counting the number of rows in a Spark DataFrame requires a sequential scan of the DataFrame. Doing this
would cause the absolute time spent running ModelDB S+C code to grow linearly with the dataset size. So, ModelDB Spark
Client disables row-counting by default.

In Figure \ref{fig:time_overhead}, the dataset size is measured in number of rows, rather than in
the actual size of the data file. This is done because the file format of the data has a big impact on
the size of the data file. Nevertheless, for reference, the largest dataset size (1 million rows) was under 300 MB
in size. All data was in CSV format.

Thus, Figure \ref{fig:time_overhead} shows that the time overhead of ModelDB S+C becomes insignificant 
as the dataset grows beyond 300 MB. 

The slowest step in recording and storing a model or operation, as determined by 
instrumenting various lines of code, is writing to the SQLite database. This is not a surprise
because the SQLite file lives on disk and because SQLite is not a very performant database.
Replacing SQLite with a more performant database may not only reduce the time overhead, it may
also reduce the storage requirements as well if data is appropriately compressed.

Focusing on the individual operations, the operation that consumed the most time, by far, was
GridSearchCrossValidationEvent. The average time overhead percentages for the most time
consuming operations are shown in Figure \ref{fig:event_time_overhead}.

\begin{figure}
  \centering
  \includegraphics[width=6.0in]{event_time_overhead}
  \caption{
    Average time overhead percentage by event.
  }
  \label{fig:event_time_overhead}
\end{figure}

Figure \ref{fig:event_time_overhead} shows that roughly 3\% of the time in the overall
program is spent storing GridSearchCrossValidationEvents. This occurs because a GridSearchCrossValidationEvent
has many other smaller events contained inside it, such as TransformEvents, FitEvents, MetricEvents, and 
CrossValidationEvents. Consequently, ModelDB Server has to write many rows to many database tables when a
GridSearchCrossValidationEvent occurs. If the user is not interested in storing all the intermediate FitEvents,
TransformEvents, etc. for a GridSearchCrossValidation, then much of this overhead is a waste. Therefore, ModelDB S+C
allows the user to configure whether they would like to actually store all the data associated with a GridSearchCrossValidationEvent,
or whether they would like to simply store the FitEvent associated with the produced model. This makes it possible to cut out
much of the overhead caused by GridSearchCrossValidationEvents.

Thus, to summarize, ModelDB S+C's time overhead is small when the dataset is large (i.e. 300 MB or more). A
large fraction of the time is spent storing GridSearchCrossValidationEvents, and since the user may not
care about storing all the intermediate TransformEvents, MetricEvents, etc., the user is allowed to indicate 
whether they would like to store the full event or or just the FitEvent for the produced model.

\section{Storage Overhead Results}
For a fixed (dataset, workflow) pair, the size of ModelDB S+C's database and the size of the
model files do not change even as the number of rows in the dataset changes. This makes sense
because, except for counting the number of rows, ModelDB S+C does not access any data from the
DataFrame's rows. The size of ModelDB S+C's SQLite database is shown for each of the (dataset, workflow)
pairs in Figure \ref{fig:dbsize}.

\begin{figure}
  \centering
  \includegraphics[height=4.0in]{dbsize}
  \caption{
    Database size (in KB) for each (dataset, workflow) pair.
  }
  \label{fig:dbsize}
\end{figure}

For all the (dataset, workflow) pairs, the database size remains under 10MB, and is
just a few MB for most of the (dataset, workflow) pairs. Since machine learning datasets
tend to be large (e.g. several hundred GB), 10MB is quite small. That being said, inspecting
the number of rows in each table for the Animal Shelter exploratory workflow 
can provide some insight into reducing the database size even further. This is shown in Figure
\ref{fig:animal_exploratory_table_sizes}.

\begin{figure}
  \centering
  \includegraphics[width=6.0in]{animal_exploratory_table_sizes}
  \caption{
    Size of tables (in number of rows) for exploratory workflow for Animal
    Shelter dataset.
  }
  \label{fig:animal_exploratory_table_sizes}
\end{figure}

Overwhelmingly, the TreeModel and TreeLink tables have the most rows. A little
thought, however, shows that this is not surprising. The Animal Shelter exploratory workflow
trains a number of random forest models. Consider
a random forest with 20 decision trees where each tree has depth 7. Further assume
that each decision tree is a binary tree. This means that each tree has on the
order of $2^{7} = 128$ nodes (and roughly the same number of links). Thus, the
random forest has $20 \times 128 = 2650$ nodes (and roughly the same number of links).
If 3-fold grid search cross validation is conducted with 8 hyperparameter configurations (a $4 \times 2$ 
search grid), and a total of 3 such grid search cross validations are performend, 
this becomes $2650 \times 8 \times 3 \times 3 = 190,800$ nodes (and roughly the same number of links).
Therefore, it is no surprise that the TreeNode and TreeLink tables are so large.

ModelDB S+C provides two mechanisms to reduce database size. First, as described before, it
allows the user to forgo storing ALL the data associated with a GridSearchCrossValidationEvent and instead
store only the most important parts.
Second, it allows the user to forgo storing entries in the TreeLink and TreeNode table (the
node and link data are already stored in the serialized model file) except for specified models. This can reduce the database size
greatly, and allow the user to only store TreeNode and TreeLink rows when it is necessary. In the 
exploratory workflow for the Animal Shelter workflow, only one (i.e. the best)  model's TreeLink and TreeNode data is really needed,
but the workflow stores data for dozens of random forest models. When the above two mechanisms are applied,
the size of the database falls from about 9 MB to 1.6 MB, as shown in figure \ref{fig:dbsize_small}.

\begin{figure}
  \centering
  \includegraphics[width=5.0in]{dbsize_small}
  \caption{
    Size of database for each (dataset, workflow) pair when GridSearchCrossValidationEvent intermediates
    are not stored and when the TreeLink and TreeNode tables are only populated for the last model.
  }
  \label{fig:dbsize_small}
\end{figure}


Finally, it is worth looking at the size of the model files produced by each (dataset, workflow) pair. This is shown in
Figure \ref{fig:modelsizes}.

\begin{figure}
  \centering
  \includegraphics[width=5.0in]{modelsizes}
  \caption{
    Size of model files for each (dataset, workflow) pair.
  }
  \label{fig:modelsizes}
\end{figure}

Again, while these sizes are small (< 5 MB) compared to the size of typical machine learning
datasets, they can still be reduced. Rather than serializing and storing every single model produced in the 
workflow, ModelDB S+C allows the user to indicate which models they would actually like to serialize and store (by
calling saveSync() on the model). When the final models are explicitly serialized and stored, rather than serializing and storing
all the produced Transformers, the model sizes in Figure \ref{fig:modelsizes_small} occur. 

\begin{figure}
  \centering
  \includegraphics[width=5.0in]{modelsizes_small}
  \caption{
    Size of model files for each (dataset, workflow) pair when only the final models are serialized
    and stored, rather than serializing and storing all produced Transformers.
  }
  \label{fig:modelsizes_small}
\end{figure}

The models in \ref{fig:modelsizes_small} take up much less space.

\section{API Method Time Results}
The IMDB exploratory workflow took about 490 seconds (a little over 8 minutes)
to execute, and the majority of this time was spent training models. Figure \ref{fig:methodtimes}
shows the time taken by various API method, as a function of the number of times each row
was duplicated (recall that the duplication was done to simulate multiple workflows).

\begin{figure}
  \centering
  \includegraphics[width=6.0in]{methodtimes}
  \caption{
    Time taken for each method, plotted as a function of the number of times that
    each row was duplicated.
  }
  \label{fig:methodtimes}
\end{figure}

Figure \ref{fig:methodtimes} shows that the running times of the API methods are
very small, with not a single API method taking more than 500 milliseconds. The fast running
times are due to the indices in ModelDB Server's database, which speed up the slow queries
and due to the fact that all of ModelDB Server's API methods run in $O(n)$ time or better, where
$n$ is the size of the database. The running time grows slowly with the number of workflows, and
should remain small (under a few seconds) even for a large number of workflows. That being said, there are still
a few potential indices that could be created in order to speed up the API methods. Second, some API
methods (e.g. descendent models) could theoretically be parallelized. Third, the overall ModelDB system includes
abstractions for grouping together Syncable Events and primitives (e.g. ExperimentRun, Project), and queries could be
restricted to run within a specific ExperimentRun or Project, because these tend to be small. Fourth,
using a more performant database than SQLite could speed up the API methods. Overall, though, the running
time of the API methods is very small compared to the time taken to run the IMDB exploratory workflow.

\section{Model Files Compared to PMML Results}
The PMML Website includes model files for a random forest model and logistic regression
model training on the Iris dataset. To compare ModelDB S+C's storage to PMML, a
a random forest model and one vs. rest logistic regression model were trained on the Iris
dataset. The results are shown in Table \ref{tab:pmml}. Note that the one vs. rest classifier
actually trains multiple logistic regression models. A one vs. rest classifier was used
because Spark.ML does not currently support the multi-class logistic regression model. To
compensate, only one logistic regression model is counted for the sizing below, rather than
all of them.

 \begin{table}
   \centering
    \begin{tabular}{ | l | l | l | l |}
      \hline
      Model Type & Database Size (KB) & Models Size (KB) & PMML Size (KB) \\ \hline
      Logistic Regression & 16 & 12 & 4 \\ \hline
      Random Forest & 116 & 224 & 420 \\ 
      \hline
   \end{tabular}
   \caption{Comparing ModelDB database and model files to PMML file}
   \label{tab:pmml}
 \end{table}

 So, for the logistic regression model, ModelDB S+C uses $16 + 12 = 28$ KB, which is
 higher than the $4$ KB required for PMML. This is because the logistic regression model
 is quite simple, which allows the PMML file to be small. ModelDB S+C must store information
 about the DataFrame, TransformerSpec, DataFrameColumn, and Hyperparameters in its database and
 it stores some metadata for the deserializer in the model file. These two factors cause ModelDB S+C
 to use more storage space than the PMML file does.

 The situation is reversed for the random forest, however. ModelDB S+C only uses $116 + 224 = 340$ KB,
 while the PMML file uses $420$ KB. The random forest is large (200 trees), so the overhead of storing the DataFrame,
 TransformerSpec, DataFrameColumn, and Hyperparameters and the overhead of storing deserialization metadata are now small.
 ModelDB is able to leverage the (somewhat) efficient storage format of SQLite and efficient storage format of Parquet (for 
 the model file), which keeps the overall storage space small. PMML, on the other hand, is an XML-like language, which becomes
 quite verbose for a large model like the random forest.

 Overall, it seems that ModelDB S+C does not require hugely more storage space than PMML for storing models. 
 While it may require more storage space in some cases, it is worth it because PMML cannot be queried like the SQLite
 database can and Spark.ML cannot deserialize PMML models and use them in the program. One advantage that PMML
 has is that it is a human readable file (The Parquet file and SQLite file are not human readable) and it is more well-known. 
 However, since ModelDB S+C is not tied to a particular storage format, the user could store their serialized model in PMML, 
 if they so choose.

\section{Evaluating Existing Workflows Results}
Three real machine learning workflows (Flight Delays, Titanic, and SMS Spam) were
considered and modified to use ModelDB S+C. This was done to see how many operations in the
machine learning workflow ModelDB S+C is able to record. Note in the following paragraphs that "captures"
means that ModelDB S+C is able to automatically record and store the operations and machine learning models
associated with a step of the workflow.

The Flight Delays workflow begins by reading the CSV file and parsing the data. These parsing
steps (e.g. convert timestamp into Java Date) are not captured by ModelDB S+C. While it is possible
for the user to indicate this parsing step by manually buffering a TransformEvent in the ModelDBSyncer,
it is not done automatically. Then, the workflow creates a pre-processing pipeline, which can be captured
completely by ModelDB S+C. Next, a decision tree model and logistic regression model are trained and
used to make predictions on the test data - this too can be captured completely by ModelDB S+C.

The Titanic workflow begins by reading the data and splitting the data into training and testing sets, which is captured by
ModelDB S+C. Then, it creates a number of visualizations - this is completely ignored by ModelDB S+C.
Next, some null values are are replaced with the median value of the column - this is not automatically 
captured by ModelDB S+C, but the user could manually log a TransformEvent to indicate this cleaning step.
Then, grid search cross validation is used to train a logistic regression model - ModelDB S+C completely 
captures this step. Next, a random forest model is trained using grid search cross validation, and this too
is completely captured by ModelDB S+C. Then, the feature importances of the models are listed - this too
is captured by ModelDB S+C (it stores feature importances for linear and tree models).

The SMS Spam workflow begins by reading the data and applying a HashingTF transformer to it - ModelDB S+C
can capture this step. Then, a logistic regression mdoel is trained, which ModelDB S+C captures. Finally,
the model is used to make some predictions, which is also captured by ModelDB S+C.

The above examples illustrate that ModelDB S+C is able to log many of the machine learning operations and 
models in actual machine learning workflows. It is not able to log any created visualizations, but that is out
of scope for ModelDB S+C. There are some data parsing and cleaning steps that it is not able to log automatically, but
it allows the user to manually buffer a TransformEvent for this purpose. In the case of the workflows above, it
may actually be possible to log some of the data parsing and cleaning steps automatically (create implicit classes for
Spark's RDD methods and log a TransformEvent when they are executed), but that has not been
implemented yet in ModelDB Spark Client.

\section{Improvements}
While a number of potential improvements have been discussed in the previous sections,
it is worth focusing on a few of them below.

First, using a better database than SQLite may improve ModelDB S+C's storage and time
overhead. SQLite is not designed for performance. It was used because it is simple and allows
ModelDB S+C to be used out of the box.

Second, ModelDB S+C misses a number of opportunities to parallelize queries. For example,
when storing CrossValidationEvents in a GridSearchCrossValidationEvent, each CrossValidationEvent
is stored one at a time. This is not strictly necessary, as they could be stored in parallel.
Writing ModelDB's storage algorithms so that they can store multiple Syncable Events or primitives 
in parallel may speed up the runtime significantly.

Third, rather than creating a TreeLink table and TreeNode table, it may be possible to store
tree models in a JSON column. Since JSON columns are not supported in SQLite, however, this was
not done.

Fourth, ModelDB does not apply any compression of its model files. Doing this can reduce their storage requirements.
